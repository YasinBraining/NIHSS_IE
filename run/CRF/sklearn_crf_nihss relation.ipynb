{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "from itertools import chain\n",
    "import nltk\n",
    "import sklearn\n",
    "import scipy. stats\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import groupby\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('训练语料/relation extraction dataset/re_train.pkl', 'rb') as f:\n",
    "    rtrain = pickle.load(f)\n",
    "with open('训练语料/relation extraction dataset/re_test.pkl', 'rb') as f:\n",
    "    rtest = pickle.load(f)\n",
    "with open('训练语料/relation extraction dataset/re_valid.pkl', 'rb') as f:\n",
    "    rvalid = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag2code(tags):\n",
    "    code=[]\n",
    "    #ent=()\n",
    "    ents=[]\n",
    "    n=0\n",
    "    m=1\n",
    "    for i in range(len(tags)):\n",
    "        #print(i,tags[i][0])\n",
    "        if tags[i][0]=='O':\n",
    "            code.append('0')\n",
    "        elif tags[i][0]=='B':\n",
    "            n+=1\n",
    "            code.append('T'+str(n))\n",
    "            \n",
    "        elif tags[i][0]=='I' and tags[i+1][0]=='I':\n",
    "            code.append('T'+str(n))\n",
    "            m+=1\n",
    "        elif tags[i][0]=='I' and tags[i+1][0]!='I':\n",
    "            code.append('T'+str(n))\n",
    "            ent=(code[len(code)-1],tags[i][2:len(tags[i])],i-m, i)\n",
    "            ents.append(ent)\n",
    "    return ents,code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taglist=rtrain[0]['tags']\n",
    "tag2code(taglist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end2end_dict_list=[]\n",
    "for i in rtrain:\n",
    "    end2end_dict={}\n",
    "    taglist=i['tags']\n",
    "    end2end_dict['HADM_ID']=i[0]\n",
    "    end2end_dict['token']=i[1]\n",
    "    end2end_dict['tags']=i[2]\n",
    "    end2end_dict['relations']=tag2code(taglist)[0]\n",
    "    end2end_dict['code']=tag2code(taglist)[1]\n",
    "    end2end_dict_list.append(end2end_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file2set(file):\n",
    "    rset=[]\n",
    "    pid=[]\n",
    "    for i in file:\n",
    "        subset=[]\n",
    "        pid.append(i['HADM_ID'])\n",
    "        code=tag2code(i['tags'])[1]\n",
    "\n",
    "        for j in range(len(i['token'])):\n",
    "            subset.append((i['token'][j],code[j],i['tags'][j]))\n",
    "        rset.append(subset)\n",
    "    return rset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtrainset=file2set(rtrain)\n",
    "rtestset=file2set(rtest)\n",
    "rvalidset=file2set(rvalid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtrainset=[]\n",
    "trainpid=[]\n",
    "for i in rtrain:\n",
    "    trainset=[]\n",
    "    trainpid.append(i['HADM_ID'])\n",
    "    code=tag2code(i['tags'])[1]\n",
    "    \n",
    "    for j in range(len(i['token'])):\n",
    "        trainset.append((i['token'][j],i['tags'][j],code[j]))\n",
    "    rtrainset.append(trainset)\n",
    "rtrainset[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtestset=[]\n",
    "testpid=[]\n",
    "for i in rtest:\n",
    "    testset=[]\n",
    "    testpid.append(i['HADM_ID'])\n",
    "    code=tag2code(i['tags'])[1]\n",
    "    \n",
    "    for j in range(len(i['token'])):\n",
    "        testset.append((i['token'][j],i['tags'][j],code[j]))\n",
    "    rtestset.append(testset)\n",
    "rtestset[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "validating set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rvalidset=[]\n",
    "validpid=[]\n",
    "for i in rvalid:\n",
    "    validset=[]\n",
    "    validpid.append(i['HADM_ID'])\n",
    "    code=tag2code(i['tags'])[1]\n",
    "    \n",
    "    for j in range(len(i['token'])):\n",
    "        validset.append((i['token'][j],i['tags'][j],code[j]))\n",
    "    rvalidset.append(validset)\n",
    "rvalidset[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent,i):\n",
    "    word=str(sent[i][0])\n",
    "    #tag= sent[i][1] \n",
    "    \n",
    "    features={\n",
    "        'bias':1.0,\n",
    "        'word.lower()':word.lower(),\n",
    "        'word[-3:]':word[-3:],\n",
    "        'word[-2:]':word[-2:],\n",
    "        'word.isupper()':word.isupper(),\n",
    "        'word.istitle()':word.istitle(),\n",
    "        'word.isdigit()':word.isdigit(),\n",
    "        #'tag-start':tag[0],\n",
    "        #'tag-end':tag.split('-')[len(tag.split('-'))-1],\n",
    "        \n",
    "    }\n",
    "    if i>0:\n",
    "        word1=str(sent[i-1][0])\n",
    "        #tag1=sent[i-1][1]\n",
    "        features.update({\n",
    "            '-1:word.lower()':word1.lower(),\n",
    "            '-1:word.istitle()':word1.istitle(),\n",
    "            '-1:word.isupper()':word1.isupper(),\n",
    "            #'tag-start':tag1[0],\n",
    "            #'tag-end':tag1.split('-')[len(tag1.split('-'))-1],\n",
    "        })\n",
    "    else:\n",
    "        features['BOS']=True\n",
    "        \n",
    "    if i<len(sent)-1:\n",
    "        word1=str(sent[i+1][0])\n",
    "        #print(i,word1)\n",
    "        #tag1=sent[i+1][1]\n",
    "        features.update({\n",
    "            '+1:word.lower()':word1.lower(),\n",
    "            '+1:word.istitle()':word1.istitle(),\n",
    "            '+1:word.isupper()':word1.isupper(),\n",
    "            #'tag-start':tag1[0],\n",
    "            #'tag-end':tag1.split('-')[len(tag1.split('-'))-1],\n",
    "        })\n",
    "    else:\n",
    "        features['EOS']=True\n",
    "    \n",
    "    return features\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent,i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, postag, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, postag, label in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_rtrain=[sent2features(s) for s in rtrainset]\n",
    "y_rtrain=[sent2labels(s) for s in rtrainset]\n",
    "\n",
    "X_rtest=[sent2features(s) for s in rtestset]\n",
    "y_rtest=[sent2labels(s) for s in rtestset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_rvalid=[sent2features(s) for s in rvalidset]\n",
    "y_rvalid=[sent2labels(s) for s in rvalidset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "crf=sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf.fit(X_rtrain, y_rtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels=list(crf.classes_)\n",
    "labels.remove('O')\n",
    "label_set=[x.split('-')[1] for x in labels]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_rpred=crf.predict(X_rtest)\n",
    "metrics.flat_f1_score(y_rtest, y_rpred, average='weighted',labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_rpred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sorted_labels=sorted(labels, key=lambda name: (name[1:], name[0]))\n",
    "print(metrics.flat_classification_report(y_rtest, y_rpred, labels=sorted_labels, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# define fixed parameters and parameters to search\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs', \n",
    "    max_iterations=100, \n",
    "    all_possible_transitions=True\n",
    ")\n",
    "params_space = {\n",
    "    'c1': scipy.stats.expon(scale=0.5),\n",
    "    'c2': scipy.stats.expon(scale=0.05),\n",
    "}\n",
    "\n",
    "# use the same metric for evaluation\n",
    "f1_scorer = make_scorer(metrics.flat_f1_score, \n",
    "                        average='weighted', labels=labels)\n",
    "\n",
    "# search\n",
    "rs = RandomizedSearchCV(crf, params_space, \n",
    "                        cv=3, \n",
    "                        verbose=1, \n",
    "                        n_jobs=-1, \n",
    "                        n_iter=50, \n",
    "                        scoring=f1_scorer)\n",
    "rs.fit(X_rtrain, y_rtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "crf = rs.best_estimator_\n",
    "y_rpred_valid = crf.predict(X_rvalid)\n",
    "print(metrics.flat_classification_report(\n",
    "    y_rvalid, y_rpred_valid, labels=sorted_labels, digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2tag(sent_list):\n",
    "    sent2tag_list=[]\n",
    "    for i in sent_list:\n",
    "        sent2tag_list=sent2tag_list+i\n",
    "    return sent2tag_list\n",
    "\n",
    "def tag2entity(sent_list):\n",
    "    tag2entity_list=[]\n",
    "    for j in sent2tag(sent_list):\n",
    "        if j=='O':\n",
    "            tag2entity_list.append([j])\n",
    "        elif j[0]=='B':\n",
    "            tag2entity_list.append([j])\n",
    "        elif j[0]=='I':\n",
    "            tag2entity_list[len(tag2entity_list)-1].append(j)\n",
    "    return tag2entity_list\n",
    "\n",
    "#def tag2entity_pred(sent_list,y_pred):\n",
    "#    tag2entity_pred_list=[]\n",
    "#    x=0\n",
    "#    for i in tag2entity(sent_list):\n",
    "#        tag2entity_pred_list.append(sent2tag(y_pred)[x:x+len(i)])\n",
    "#        x+=len(i)\n",
    "#    return tag2entity_pred_list\n",
    "        \n",
    "def entity2label(tag2entity_list):\n",
    "    entity2label_list=[]\n",
    "    for k in tag2entity_list:\n",
    "        #print(k)\n",
    "        l=[]\n",
    "        for m in k:\n",
    "            lm=m.split('-')\n",
    "            l.append(lm[len(lm)-1])\n",
    "        if len(set(l))==1:\n",
    "            entity2label_list.append(l[0])\n",
    "        elif len(set(l))>1:\n",
    "            entity2label_list.append(','.join(set(l)))  \n",
    "            #print(','.join(set(l)))\n",
    "    return entity2label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag2entity_pred(sent_list_test, sent_list_pred):\n",
    "    tag2entity_pred_list=[]\n",
    "    tag2entity_list=tag2entity(sent_list_test)\n",
    "    sent2tag_pred_list=sent2tag(sent_list_pred)\n",
    "    x=0\n",
    "    for i in tag2entity_list:\n",
    "        tag2entity_pred_list.append(sent2tag_pred_list[x:x+len(i)])\n",
    "        x+=len(i)\n",
    "    return tag2entity_pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag2entity_pred0_list=[]\n",
    "tag2entity_list=tag2entity(y_rtest)\n",
    "sent2tag_pred0_list=sent2tag(y_rpred)\n",
    "x=0\n",
    "for i in tag2entity_list:\n",
    "    tag2entity_pred0_list.append(sent2tag_pred0_list[x:x+len(i)])\n",
    "    x+=len(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test0_label=entity2label(tag2entity(y_rtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred0_label=entity2label(tag2entity_pred0_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_test0_label),len(y_pred0_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test0_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test0_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate precision recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "precision: corrected predicted nihss entitiy / all entity predicted as nihss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP_dict=dict()\n",
    "PP_dict=dict()\n",
    "RP_dict=dict()\n",
    "for i in [TP_dict,PP_dict, RP_dict]:\n",
    "    for j in label_set:\n",
    "        i[j]=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TP: number of entities correctly predicted as A,\n",
    "* PP: number of entities predicted as A, correct or not, \n",
    "* RP: real number of entities that is A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(y_test0_label)):\n",
    "    if y_test0_label[i]==y_pred0_label[i] and y_test0_label[i]!='O':\n",
    "        TP_dict[y_test0_label[i]]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in y_pred0_label:\n",
    "    if i in label_set:\n",
    "        PP_dict[i]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in y_test0_label:\n",
    "    if i in label_set:\n",
    "        RP_dict[i]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_dict=dict()\n",
    "recall_dict=dict()\n",
    "f1_dict=dict()\n",
    "for i in label_set:\n",
    "    precision_dict[i]=TP_dict[i]/PP_dict[i]\n",
    "    recall_dict[i]=TP_dict[i]/RP_dict[i]\n",
    "    f1_dict[i]=2*(precision_dict[i]*recall_dict[i])/(precision_dict[i]+recall_dict[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_df=pd.DataFrame(precision_dict, index=['precision'])\n",
    "recall_df=pd.DataFrame(recall_dict, index=['recall'])\n",
    "f1_df=pd.DataFrame(f1_dict, index=['f1'])\n",
    "pd.concat([precision_df, recall_df,f1_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overall_perf(test_label,pred_label):\n",
    "    tp=0\n",
    "    for i in range(len(test_label)):\n",
    "        if test_label[i]==pred_label[i] and test_label[i]!='O':\n",
    "            tp+=1\n",
    "    while 'O' in test_label:\n",
    "        test_label.remove('O')\n",
    "    rp=len(test_label)\n",
    "    \n",
    "    while 'O' in pred_label:\n",
    "        pred_label.remove('O')\n",
    "    pp=len(pred_label)\n",
    "    precision=tp/pp\n",
    "    recall=tp/rp\n",
    "    f1=2*(precision*recall)/(precision+recall)\n",
    "    overall_df=pd.DataFrame(data={'precision':precision, 'recall':recall, 'f1':f1}, index=['overall'])\n",
    "    return overall_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(traindf,testdf):\n",
    "    train_set=df2list(traindf)\n",
    "    test_set=df2list(testdf)\n",
    "    \n",
    "    X_train=[sent2features(s) for s in train_set]\n",
    "    y_train=[sent2labels(s) for s in train_set]\n",
    "    X_test=[sent2features(s) for s in test_set]\n",
    "    y_test=[sent2labels(s) for s in test_set]\n",
    "    \n",
    "    crf=sklearn_crfsuite.CRF(\n",
    "        algorithm='lbfgs',\n",
    "        c1=0.1,\n",
    "        c2=0.1,\n",
    "        max_iterations=100,\n",
    "        all_possible_transitions=True\n",
    "    )\n",
    "    crf.fit(X_train, y_train)\n",
    "    \n",
    "    labels=list(crf.classes_)\n",
    "    labels.remove('O')\n",
    "    label_set=[x.split('-')[1] for x in labels]\n",
    "\n",
    "    y_pred=crf.predict(X_test)\n",
    "    f1_score=metrics.flat_f1_score(y_test, y_pred, average='weighted',labels=labels)\n",
    "    print('f1:',f1_score)\n",
    "    \n",
    "    y_test_label=entity2label(tag2entity(y_test))\n",
    "    y_pred_label=entity2label(tag2entity_pred(y_test, y_pred))\n",
    "    \n",
    "    TP_dict=dict()\n",
    "    PP_dict=dict()\n",
    "    RP_dict=dict()\n",
    "    for i in [TP_dict,PP_dict, RP_dict]:\n",
    "        for j in label_set:\n",
    "            i[j]=0\n",
    "    \n",
    "    #print(y_test_label[3292])\n",
    "    for l in range(len(y_test_label)):\n",
    "        if y_test_label[l]==y_pred_label[l] and y_test_label[l]!='O':\n",
    "            #print(l,y_test_label[l])\n",
    "            TP_dict[y_test_label[l]]+=1\n",
    "    \n",
    "    for m in y_pred_label:\n",
    "        if m in label_set:\n",
    "            PP_dict[m]+=1\n",
    "            \n",
    "    for n in y_test_label:\n",
    "        if n in label_set:\n",
    "            RP_dict[n]+=1\n",
    "            \n",
    "    precision_dict=dict()\n",
    "    recall_dict=dict()\n",
    "    f1_dict=dict()\n",
    "    for s in label_set:\n",
    "        precision_dict[s]=TP_dict[s]/PP_dict[s]\n",
    "        recall_dict[s]=TP_dict[s]/RP_dict[s]\n",
    "        f1_dict[s]=2*(precision_dict[s]*recall_dict[s])/(precision_dict[s]+recall_dict[s])\n",
    "    \n",
    "    precision_df=pd.DataFrame(precision_dict, index=['precision'])\n",
    "    recall_df=pd.DataFrame(recall_dict, index=['recall'])\n",
    "    f1_df=pd.DataFrame(f1_dict, index=['f1'])\n",
    "    performance=pd.concat([precision_df, recall_df,f1_df])\n",
    "    performancet=pd.DataFrame(performance.values.T, index=performance.columns, columns=performance.index)\n",
    "    #print('len test',len(y_test_label),'len pred',len(y_pred_label))\n",
    "    overalldf=overall_perf(y_test_label,y_pred_label)\n",
    "    performancet=performancet.append(overalldf)\n",
    "    return performancet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_test(traindf0,testdf0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter('performance.xlsx')\n",
    "\n",
    "for i,j,k in [(traindf0,testdf0,'fold0'),(traindf1,testdf1,'fold1'),(traindf2,testdf2,'fold2'),(traindf3,testdf3,'fold3'),(traindf4,testdf4,'fold4')]:\n",
    "    df=train_test(i,j)\n",
    "    df.to_excel(writer, sheet_name=k)\n",
    "    print(k)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_vali(traindf,testdf,validf):\n",
    "    train_set=df2list(traindf)\n",
    "    test_set=df2list(testdf)\n",
    "    vali_set=df2list(validf)\n",
    "    \n",
    "    X_train=[sent2features(s) for s in train_set]\n",
    "    y_train=[sent2labels(s) for s in train_set]\n",
    "    X_test=[sent2features(s) for s in test_set]\n",
    "    y_test=[sent2labels(s) for s in test_set]\n",
    "    X_vali=[sent2features(s) for s in vali_set]\n",
    "    y_vali=[sent2labels(s) for s in vali_set]\n",
    "    \n",
    "    crf=sklearn_crfsuite.CRF(\n",
    "        algorithm='lbfgs',\n",
    "        c1=0.1,\n",
    "        c2=0.1,\n",
    "        max_iterations=100,\n",
    "        all_possible_transitions=True\n",
    "    )\n",
    "    crf.fit(X_train, y_train)\n",
    "    \n",
    "    labels=list(crf.classes_)\n",
    "    labels.remove('O')\n",
    "    label_set=[x.split('-')[1] for x in labels]\n",
    "    \n",
    "    crf = sklearn_crfsuite.CRF(\n",
    "        algorithm='lbfgs', \n",
    "        max_iterations=100, \n",
    "        all_possible_transitions=True\n",
    "    )\n",
    "    params_space = {\n",
    "        'c1': scipy.stats.expon(scale=0.5),\n",
    "        'c2': scipy.stats.expon(scale=0.05),\n",
    "    }\n",
    "    \n",
    "    # use the same metric for evaluation\n",
    "    f1_scorer = make_scorer(metrics.flat_f1_score, \n",
    "                            average='weighted', labels=label_set)\n",
    "\n",
    "    # search\n",
    "    rs = RandomizedSearchCV(crf, params_space, \n",
    "                            #cv=3, \n",
    "                            verbose=1, \n",
    "                            n_jobs=-1, \n",
    "                            n_iter=50, \n",
    "                            scoring=f1_scorer)\n",
    "    rs.fit(X_train, y_train)\n",
    "    crf=rs.best_estimator_\n",
    "    y_pred_vali=crf.predict(X_vali)\n",
    "\n",
    "    #y_pred=crf.predict(X_test)\n",
    "    #f1_score=metrics.flat_f1_score(y_test, y_pred, average='weighted',labels=labels)\n",
    "    #print('f1:',f1_score)\n",
    "    \n",
    "    y_vali_label=entity2label(tag2entity(y_vali))\n",
    "    y_pred_vali_label=entity2label(tag2entity_pred(y_vali, y_pred_vali))\n",
    "    \n",
    "    performance=perfm(y_vali_label,y_pred_vali_label)\n",
    "    overalldf=overall_perf(y_vali_label,y_pred_vali_label)\n",
    "    performance=performance.append(overalldf)\n",
    "    \n",
    "    return performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_vali(traindf1,testdf1,validf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=0\n",
    "for i,j,k in [(traindf0,testdf0,'fold0'),(traindf1,testdf1,'fold1'),(traindf2,testdf2,'fold2'),(traindf3,testdf3,'fold3'),(traindf4,testdf4,'fold4')]:\n",
    "    print(y)\n",
    "    if y==0:\n",
    "        df=train_test_vali(i,j,validf)\n",
    "        y+=1\n",
    "    else:\n",
    "        dfy=train_test_vali(i,j,validf)\n",
    "        df=pd.concat([df,dfy], axis=1, join_axes=[df.index])\n",
    "        y+=1\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('performance_vali.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# define fixed parameters and parameters to search\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs', \n",
    "    max_iterations=100, \n",
    "    all_possible_transitions=True\n",
    ")\n",
    "params_space = {\n",
    "    'c1': scipy.stats.expon(scale=0.5),\n",
    "    'c2': scipy.stats.expon(scale=0.05),\n",
    "}\n",
    "\n",
    "# use the same metric for evaluation\n",
    "f1_scorer = make_scorer(metrics.flat_f1_score, \n",
    "                        average='weighted', labels=labels)\n",
    "\n",
    "# search\n",
    "rs = RandomizedSearchCV(crf, params_space, \n",
    "                        cv=3, \n",
    "                        verbose=1, \n",
    "                        n_jobs=-1, \n",
    "                        n_iter=50, \n",
    "                        scoring=f1_scorer)\n",
    "rs.fit(X_train0, y_train0)\n",
    "crf=rs.best_estimator_\n",
    "y_pred_t=crf.predict(X_vali)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perfm(y_real,y_pred):\n",
    "    TP_dict=dict()\n",
    "    PP_dict=dict()\n",
    "    RP_dict=dict()\n",
    "    for i in [TP_dict,PP_dict, RP_dict]:\n",
    "        for j in label_set:\n",
    "            i[j]=0\n",
    "    \n",
    "    for i in range(len(y_real)):\n",
    "        if y_real[i]==y_pred[i] and y_real[i]!='O':\n",
    "            TP_dict[y_real[i]]+=1\n",
    "    for i in y_pred:\n",
    "        if i in label_set:\n",
    "            PP_dict[i]+=1\n",
    "    #print(PP_dict)\n",
    "    for i in y_real:\n",
    "        if i in label_set:\n",
    "            RP_dict[i]+=1\n",
    "            \n",
    "    precision_dict=dict()\n",
    "    recall_dict=dict()\n",
    "    f1_dict=dict()\n",
    "    for i in label_set:\n",
    "        if PP_dict[i]==0:\n",
    "            precision_dict[i]=0\n",
    "        else:   \n",
    "            precision_dict[i]=TP_dict[i]/PP_dict[i]\n",
    "        if RP_dict[i]==0:\n",
    "            recall_dict[i]=0\n",
    "        else:\n",
    "            recall_dict[i]=TP_dict[i]/RP_dict[i]\n",
    "        if (precision_dict[i]+recall_dict[i])==0:\n",
    "            f1_dict[i]=0\n",
    "        else:\n",
    "            f1_dict[i]=2*(precision_dict[i]*recall_dict[i])/(precision_dict[i]+recall_dict[i])\n",
    "    \n",
    "    precision_df=pd.DataFrame(precision_dict, index=['precision'])\n",
    "    recall_df=pd.DataFrame(recall_dict, index=['recall'])\n",
    "    f1_df=pd.DataFrame(f1_dict, index=['f1'])\n",
    "    perf_df=pd.concat([precision_df, recall_df,f1_df])\n",
    "    perf_dft=pd.DataFrame(perf_df.values.T, index=perf_df.columns, columns=perf_df.index)\n",
    "    return perf_dft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_vali_list=entity2label(tag2entity(y_vali))\n",
    "y_pred_t_list=entity2label(tag2entity_pred(y_vali, y_pred_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perfm(y_vali_list,y_pred_t_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
